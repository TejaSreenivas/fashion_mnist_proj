{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import new_model as model\n",
    "import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset (cifar-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_image(x):\n",
    "    x = np.transpose(x,[0,2,3,1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 30000\n",
    "n_random = np.random.randint(0,50000,n)\n",
    "n_random.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_y = np.array([0]*n)\n",
    "noise_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "#pick n random noise images\n",
    "\n",
    "data['train_x'],data['train_y'], data['test_x'], data['test_y'] = load_mnist.load_mnist_dataset(os.getcwd()+\"/mnist\")\n",
    "noise,_,_,_ = load_mnist.load_mnist_dataset(os.getcwd()+\"/fashion\")\n",
    "data['train_x'] = np.vstack((data['train_x'],np.take(noise,n_random,axis=0)))\n",
    "data['train_y'] = np.hstack((data['train_y'],noise_y))\n",
    "#data = load_data('/floyd/input/cifar_10_batches_py/cifar-10-batches-py')\n",
    "data['train_x'] = reshape_image(data['train_x'])\n",
    "data['test_x'] = reshape_image(data['test_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train_x', 'train_y', 'test_x', 'test_y'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((90000, 28, 28, 1), (90000,)), ((10000, 28, 28, 1), (10000,)))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data['train_x'].shape,data['train_y'].shape),(data['test_x'].shape,data['test_y'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 28\n",
    "CH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c3f401b898>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.random.randint(50000)\n",
    "print('Label: {}'.format(data['train_y'][x]))\n",
    "plt.imshow(data['train_x'][x].reshape(DIM,DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  One hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (10000, 10))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = 10\n",
    "id_mtx = np.identity(n_classes,dtype=np.float32)\n",
    "data['train_y'] = id_mtx[data['train_y']]\n",
    "data['test_y'] = id_mtx[data['test_y']]\n",
    "data['train_y'].shape, data['test_y'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train_x'] = data['train_x'].astype(np.float32)/255\n",
    "data['test_x'] = data['test_x'].astype(np.float32)/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<BatchDataset shapes: ((?, 28, 28, 1), (?, 10)), types: (tf.float32, tf.float32)>,\n",
       " <BatchDataset shapes: ((?, 28, 28, 1), (?, 10)), types: (tf.float32, tf.float32)>,\n",
       " <BatchDataset shapes: ((?, 28, 28, 1), (?, 10)), types: (tf.float32, tf.float32)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.variable_scope(\"dataset_gen\" ):    \n",
    "    dtrain_x = tf.data.Dataset.from_tensor_slices(data['train_x'])\n",
    "    dtrain_y = tf.data.Dataset.from_tensor_slices(data['train_y'])\n",
    "    dtrain = tf.data.Dataset.zip(( dtrain_x, dtrain_y )).batch(batch_size)\n",
    "    \n",
    "    dtest_x = tf.data.Dataset.from_tensor_slices(data['test_x'])\n",
    "    dtest_y = tf.data.Dataset.from_tensor_slices(data['test_y'])\n",
    "    dtest = tf.data.Dataset.zip(( dtest_x,dtest_y )).batch(batch_size)\n",
    "    \n",
    "    dvalid_x = tf.data.Dataset.from_tensor_slices(data['test_x'][:1000,:,:,:])\n",
    "    dvalid_y = tf.data.Dataset.from_tensor_slices(data['test_y'][:1000])\n",
    "    dvalid = tf.data.Dataset.zip(( dtest_x,dtest_y )).batch(batch_size)\n",
    "(dtrain,dtest,dvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"dataset_init\"):\n",
    "    iterator = tf.data.Iterator.from_structure(dtrain.output_types,dtrain.output_shapes)\n",
    "    get_batch = iterator.get_next()\n",
    "    #for train\n",
    "    dtrain_init = iterator.make_initializer(dtrain)\n",
    "    #for test\n",
    "    dtest_init = iterator.make_initializer(dtest)\n",
    "    #for validation\n",
    "    dvalid_init = iterator.make_initializer(dvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10  \n",
    "probability_keep = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "varname : conv_1/weight:0 [Dimension(3), Dimension(3), Dimension(1), Dimension(32)]\n",
      "varname : conv_1/bias:0 [Dimension(32)]\n",
      "varname : conv_1/batch_normalization/gamma:0 [Dimension(32)]\n",
      "varname : conv_1/batch_normalization/beta:0 [Dimension(32)]\n",
      "varname : conv_2/weight:0 [Dimension(3), Dimension(3), Dimension(32), Dimension(64)]\n",
      "varname : conv_2/bias:0 [Dimension(64)]\n",
      "varname : batch_normalization/gamma:0 [Dimension(3136)]\n",
      "varname : batch_normalization/beta:0 [Dimension(3136)]\n",
      "varname : dense/kernel:0 [Dimension(3136), Dimension(1024)]\n",
      "varname : dense/bias:0 [Dimension(1024)]\n",
      "varname : batch_normalization_1/gamma:0 [Dimension(1024)]\n",
      "varname : batch_normalization_1/beta:0 [Dimension(1024)]\n",
      "varname : dense_1/kernel:0 [Dimension(1024), Dimension(10)]\n",
      "varname : dense_1/bias:0 [Dimension(10)]\n",
      "total number of trainable parameter : 3249738\n"
     ]
    }
   ],
   "source": [
    "is_train = tf.placeholder(shape=(),dtype=tf.bool,name='is_train')\n",
    "prob_keep = tf.placeholder(shape=(),dtype=tf.float32,name = 'probability_keep')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "current_model = model.CNN(get_batch[0], is_train, prob_keep,n_classes)\n",
    "param_info = current_model.total_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_1/BiasAdd:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits=current_model.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lr = 1e-3\n",
    "end_lr = 5e-3\n",
    "decay_steps = 10000\n",
    "lr = tf.train.exponential_decay(start_lr,global_step, decay_steps, 0.96, staircase=True)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=get_batch[1],logits=logits))\n",
    "tf.summary.scalar(\"losses\",loss)\n",
    "tf.summary.scalar(\"learning_rate\",lr)\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(lr).minimize(loss, global_step=global_step)\n",
    "predictions = tf.argmax(logits,axis=1)\n",
    "predictions = tf.Print(predictions,[predictions],\"prediction : \")\n",
    "labels = tf.argmax(get_batch[1],axis=1)\n",
    "labels = tf.Print(labels,[labels],\"the labels  : \")\n",
    "equality = tf.equal(predictions,labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(equality,tf.float32))\n",
    "conf_mtx=tf.confusion_matrix(labels=tf.argmax(get_batch[1],axis=1),predictions=tf.argmax(logits,axis=1),num_classes=n_classes)\n",
    "tf.summary.scalar(\"accuracy\",accuracy)\n",
    "#tf.summary.text(\"confusion_matrix\", tf.cast(conf_mtx,tf.string))\n",
    "ginit_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "floyd/home/logs\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'floyd/home/logs'\n",
    "print(log_dir)\n",
    "t_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dataset(sess, init_op, variables, feed_dict, train=False):\n",
    "    sess.run(init_op)\n",
    "    logs = []\n",
    "    itr = 1\n",
    "    #stack = np.random.rand(1,n_classes)\n",
    "    #stack = []\n",
    "    stack = np.zeros((10,10),dtype=np.int32)\n",
    "    while True:\n",
    "        try:\n",
    "            res = sess.run(variables, feed_dict=feed_dict)\n",
    "            logs.append(res[0]) # first element of the result is either accuracy or loss\n",
    "            if train:\n",
    "                writer.add_summary(res[-1],i)\n",
    "                itr+=1\n",
    "                if itr%100 == 0:\n",
    "                    print(\"batch :{}, loss :{:.3f}, accuracy :{:.3f}\".format(itr,res[0],res[2]))\n",
    "            else:\n",
    "                #stack = np.vstack((stack,res[-1]))\n",
    "                #stack.append(res[-1])\n",
    "                stack = stack + res[-1]\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            return logs,stack\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0325cd6f5014>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mginit_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtrain_log_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mvalid_log_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "valid_log_acc = []\n",
    "train_log_acc=[]\n",
    "test_acc=[]\n",
    "train_acc=[]\n",
    "with tf.Session() as sess:\n",
    "    writer.add_graph(sess.graph)\n",
    "    sess.run(ginit_op)\n",
    "    train_log_loss = []\n",
    "    valid_log_loss = []\n",
    "    for i in range(1,epochs+1):\n",
    "        print(\"training epoch : {}\".format(i))\n",
    "        sess.run(dtrain_init)\n",
    "        feed_dict={is_train:True,prob_keep:0.3}\n",
    "        variables = [loss,optimizer,accuracy,t_summary]\n",
    "        l,_  = run_dataset(sess, dtrain_init, variables, feed_dict, True)\n",
    "        train_log_loss.append(l)\n",
    "        \n",
    "        variables = [accuracy,conf_mtx]\n",
    "        feed_dict = {is_train:False,prob_keep:1.0}\n",
    "        acc,_ = run_dataset(sess, dvalid_init, variables, feed_dict)\n",
    "        acc = np.array(acc)\n",
    "        valid_log_acc.append(acc.mean())\n",
    "        print(\"average validation accuracy :{:.4f}\".format(acc.mean()))\n",
    "    \n",
    "    #for test dataset\n",
    "    variables = [accuracy,conf_mtx]\n",
    "    feed_dict = {is_train:False,prob_keep:1.0}\n",
    "    acc, test_stack = run_dataset(sess, dtest_init, variables, feed_dict)\n",
    "    test_acc = np.array(acc)\n",
    "    print(\"average test accuracy :{:.4f}\".format(test_acc.mean()))\n",
    "    \n",
    "    # for train dataset\n",
    "    variables = [accuracy,conf_mtx]\n",
    "    feed_dict = {is_train:False,prob_keep:1.0}\n",
    "    acc, train_stack = run_dataset(sess, dtrain_init, variables, feed_dict)\n",
    "    train_acc = np.array(acc)\n",
    "    print(\"average train accuracy :{:.2f}\".format(train_acc.mean()))\n",
    "    plt.plot(valid_log_acc,label='valid_acc',color='g')\n",
    "    train_log_loss = np.array(train_log_loss).reshape(-1)\n",
    "    x = np.arange(1,train_log_loss.shape[0]+1)/500\n",
    "    plt.plot(x,train_log_loss,label='train loss', color='r')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = dict()\n",
    "output_dict['batch_size'] = batch_size\n",
    "output_dict['epochs'] = epochs\n",
    "output_dict['loss'] = np.array(train_log_loss)\n",
    "output_dict['accuracy'] = np.array(valid_log_acc)\n",
    "output_dict['test_accuracy'] = np.array(test_acc)\n",
    "output_dict['train_accuracy'] = np.array(train_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.save('data_info_epochs_1.npy',output_dict)\n",
    "np.savetxt('train_stack.csv',train_stack[1:],delimiter=',')\n",
    "np.savetxt('test_stack.csv',test_stack[1:],delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save('/floyd/home/data_info_epoch_1.npy',output_dict)\n",
    "np.savetxt('/floyd/home/train_stack.csv',train_stack,delimiter=',')\n",
    "np.savetxt('/floyd/home/test_stack.csv',test_stack,delimiter=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
