{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import new_model as model\n",
    "import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset (cifar-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_image(x):\n",
    "    x = np.transpose(x,[0,2,3,1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 10\n",
      "19 10\n",
      "19 10\n",
      "19 10\n"
     ]
    }
   ],
   "source": [
    "from prep import mnist_funct, fashion_funct, merger\n",
    "mnist = mnist_funct()\n",
    "fashion = fashion_funct()\n",
    "merged = merger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((120000, 28, 28, 1), (120000,)), ((10000, 28, 28, 1), (10000,)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(merged['train_x'].shape,merged['train_y'].shape),(mnist['test_x'].shape,mnist['test_y'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 28\n",
    "CH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23f0d48ffd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.random.randint(50000)\n",
    "print('Label: {}'.format(merged['train_y'][x]))\n",
    "plt.imshow(merged['train_x'][x].reshape(DIM,DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  One hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((120000, 20), (20000, 20))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = 20\n",
    "id_mtx = np.identity(n_classes,dtype=np.float32)\n",
    "mnist['train_y'] = id_mtx[mnist['train_y']]\n",
    "mnist['test_y'] = id_mtx[mnist['test_y']]\n",
    "merged['train_y'] = id_mtx[merged['train_y']]\n",
    "merged['test_y'] = id_mtx[merged['test_y']]\n",
    "fashion['train_y'] = id_mtx[fashion['train_y']]\n",
    "fashion['test_y'] = id_mtx[fashion['test_y']]\n",
    "\n",
    "\n",
    "merged['train_y'].shape, merged['test_y'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged['train_x'] = merged['train_x'].astype(np.float32)\n",
    "merged['test_x'] = merged['test_x'].astype(np.float32)\n",
    "merged['train_x'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<BatchDataset shapes: ((?, 28, 28, 1), (?, 20)), types: (tf.float32, tf.float32)>,\n",
       " <BatchDataset shapes: ((?, 28, 28, 1), (?, 20)), types: (tf.float32, tf.float32)>,\n",
       " <BatchDataset shapes: ((?, 28, 28, 1), (?, 20)), types: (tf.float32, tf.float32)>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.variable_scope(\"dataset_gen\" ):    \n",
    "    dtrain_x = tf.data.Dataset.from_tensor_slices(merged['train_x'])\n",
    "    dtrain_y = tf.data.Dataset.from_tensor_slices(merged['train_y'])\n",
    "    dtrain = tf.data.Dataset.zip(( dtrain_x, dtrain_y )).shuffle(1000).batch(batch_size)\n",
    "    \n",
    "    dtest_x = tf.data.Dataset.from_tensor_slices(mnist['test_x'])\n",
    "    dtest_y = tf.data.Dataset.from_tensor_slices(mnist['test_y'])\n",
    "    dtest = tf.data.Dataset.zip(( dtest_x,dtest_y )).batch(batch_size)\n",
    "    \n",
    "    dvalid_x = tf.data.Dataset.from_tensor_slices(mnist['test_x'][:1000,:,:,:])\n",
    "    dvalid_y = tf.data.Dataset.from_tensor_slices(mnist['test_y'][:1000])\n",
    "    dvalid = tf.data.Dataset.zip(( dtest_x,dtest_y )).batch(batch_size)\n",
    "(dtrain,dtest,dvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"dataset_init\"):\n",
    "    iterator = tf.data.Iterator.from_structure(dtrain.output_types,dtrain.output_shapes)\n",
    "    get_batch = iterator.get_next()\n",
    "    #for train\n",
    "    dtrain_init = iterator.make_initializer(dtrain)\n",
    "    #for test\n",
    "    dtest_init = iterator.make_initializer(dtest)\n",
    "    #for validation\n",
    "    dvalid_init = iterator.make_initializer(dvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10  \n",
    "probability_keep = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CNN' object has no attribute 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-61d5a491338c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprob_keep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'probability_keep'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcurrent_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob_keep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mparam_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\TEJA\\ML_resources\\clg_project\\floyd\\Model\\fashion_mnist_proj\\new_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, is_train, prob_keep, classes)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob_keep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob_keep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\TEJA\\ML_resources\\clg_project\\floyd\\Model\\fashion_mnist_proj\\new_model.py\u001b[0m in \u001b[0;36mcnn\u001b[1;34m(self, data, is_train, prob_keep)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m#h1 = self.hidden_layer(100,h1,prob_keep,is_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;31m#h1 = self.hidden_layer(500,h1,prob_keep,is_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[1;31m#y = tf.layers.batch_normalization(inputs = y, training = is_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CNN' object has no attribute 'classes'"
     ]
    }
   ],
   "source": [
    "is_train = tf.placeholder(shape=(),dtype=tf.bool,name='is_train')\n",
    "prob_keep = tf.placeholder(shape=(),dtype=tf.float32,name = 'probability_keep')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "current_model = model.CNN(get_batch[0], is_train, prob_keep,n_classes)\n",
    "param_info = current_model.total_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_1/BiasAdd:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits=current_model.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lr = 1e-3\n",
    "end_lr = 5e-3\n",
    "decay_steps = 10000\n",
    "lr = tf.train.exponential_decay(start_lr,global_step, decay_steps, 0.96, staircase=True)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=get_batch[1],logits=logits))\n",
    "tf.summary.scalar(\"losses\",loss)\n",
    "tf.summary.scalar(\"learning_rate\",lr)\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(lr).minimize(loss, global_step=global_step)\n",
    "predictions = tf.argmax(logits,axis=1)\n",
    "predictions = tf.Print(predictions,[predictions],\"prediction : \")\n",
    "labels = tf.argmax(get_batch[1],axis=1)\n",
    "labels = tf.Print(labels,[labels],\"the labels  : \")\n",
    "equality = tf.equal(predictions,labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(equality,tf.float32))\n",
    "conf_mtx=tf.confusion_matrix(labels=tf.argmax(get_batch[1],axis=1),predictions=tf.argmax(logits,axis=1),num_classes=n_classes)\n",
    "tf.summary.scalar(\"accuracy\",accuracy)\n",
    "#tf.summary.text(\"confusion_matrix\", tf.cast(conf_mtx,tf.string))\n",
    "ginit_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "floyd/home/logs\n"
     ]
    }
   ],
   "source": [
    "log_dir = 'floyd/home/logs'\n",
    "print(log_dir)\n",
    "t_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dataset(sess, init_op, variables, feed_dict, train=False):\n",
    "    sess.run(init_op)\n",
    "    logs = []\n",
    "    itr = 1\n",
    "    #stack = np.random.rand(1,n_classes)\n",
    "    #stack = []\n",
    "    stack = np.zeros((10,10),dtype=np.int32)\n",
    "    while True:\n",
    "        try:\n",
    "            res = sess.run(variables, feed_dict=feed_dict)\n",
    "            logs.append(res[0]) # first element of the result is either accuracy or loss\n",
    "            if train:\n",
    "                writer.add_summary(res[-1],i)\n",
    "                itr+=1\n",
    "                if itr%100 == 0:\n",
    "                    print(\"batch :{}, loss :{:.3f}, accuracy :{:.3f}\".format(itr,res[0],res[2]))\n",
    "            else:\n",
    "                #stack = np.vstack((stack,res[-1]))\n",
    "                #stack.append(res[-1])\n",
    "                stack = stack + res[-1]\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            return logs,stack\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0325cd6f5014>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mginit_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtrain_log_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mvalid_log_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "valid_log_acc = []\n",
    "train_log_acc=[]\n",
    "test_acc=[]\n",
    "train_acc=[]\n",
    "with tf.Session() as sess:\n",
    "    writer.add_graph(sess.graph)\n",
    "    sess.run(ginit_op)\n",
    "    train_log_loss = []\n",
    "    valid_log_loss = []\n",
    "    for i in range(1,epochs+1):\n",
    "        print(\"training epoch : {}\".format(i))\n",
    "        sess.run(dtrain_init)\n",
    "        feed_dict={is_train:True,prob_keep:0.3}\n",
    "        variables = [loss,optimizer,accuracy,t_summary]\n",
    "        l,_  = run_dataset(sess, dtrain_init, variables, feed_dict, True)\n",
    "        train_log_loss.append(l)\n",
    "        \n",
    "        variables = [accuracy,conf_mtx]\n",
    "        feed_dict = {is_train:False,prob_keep:1.0}\n",
    "        acc,_ = run_dataset(sess, dvalid_init, variables, feed_dict)\n",
    "        acc = np.array(acc)\n",
    "        valid_log_acc.append(acc.mean())\n",
    "        print(\"average validation accuracy :{:.2f}\".format(acc.mean()))\n",
    "    \n",
    "    #for test dataset\n",
    "    variables = [accuracy,conf_mtx]\n",
    "    feed_dict = {is_train:False,prob_keep:1.0}\n",
    "    acc, test_stack = run_dataset(sess, dtest_init, variables, feed_dict)\n",
    "    test_acc = np.array(acc)\n",
    "    print(\"average test accuracy :{:.2f}\".format(test_acc.mean()))\n",
    "    \n",
    "    # for train dataset\n",
    "    variables = [accuracy,conf_mtx]\n",
    "    feed_dict = {is_train:False,prob_keep:1.0}\n",
    "    acc, train_stack = run_dataset(sess, dtrain_init, variables, feed_dict)\n",
    "    train_acc = np.array(acc)\n",
    "    print(\"average train accuracy :{:.2f}\".format(train_acc.mean()))\n",
    "    plt.plot(valid_log_acc,label='valid_acc',color='g')\n",
    "    train_log_loss = np.array(train_log_loss).reshape(-1)\n",
    "    x = np.arange(1,train_log_loss.shape[0]+1)/500\n",
    "    plt.plot(x,train_log_loss,label='train loss', color='r')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = dict()\n",
    "output_dict['batch_size'] = batch_size\n",
    "output_dict['epochs'] = epochs\n",
    "output_dict['loss'] = np.array(train_log_loss)\n",
    "output_dict['accuracy'] = np.array(valid_log_acc)\n",
    "output_dict['test_accuracy'] = np.array(test_acc)\n",
    "output_dict['train_accuracy'] = np.array(train_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.save('data_info_epochs_1.npy',output_dict)\n",
    "np.savetxt('train_stack.csv',train_stack[1:],delimiter=',')\n",
    "np.savetxt('test_stack.csv',test_stack[1:],delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save('/floyd/home/data_info_epoch_1.npy',output_dict)\n",
    "np.savetxt('/floyd/home/train_stack.csv',train_stack,delimiter=',')\n",
    "np.savetxt('/floyd/home/test_stack.csv',test_stack,delimiter=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
