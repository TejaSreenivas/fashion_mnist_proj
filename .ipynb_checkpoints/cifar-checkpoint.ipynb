{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import new_model as model\n",
    "import load_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset (cifar-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_image(x):\n",
    "    x = np.transpose(x,[0,2,3,1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "data['train_x'],data['train_y'], data['test_x'], data['test_y'] = load_mnist.load_mnist_dataset(os.getcwd()+\"/mnist\")\n",
    "#data = load_data('/floyd/input/cifar_10_batches_py/cifar-10-batches-py')\n",
    "data['train_x'] = reshape_image(data['train_x'])\n",
    "data['test_x'] = reshape_image(data['test_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train_x', 'train_y', 'test_x', 'test_y'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((60000, 28, 28, 1), (60000,)), ((10000, 28, 28, 1), (10000,)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data['train_x'].shape,data['train_y'].shape),(data['test_x'].shape,data['test_y'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 28\n",
    "CH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2038121cf28>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.random.randint(50000)\n",
    "print('Label: {}'.format(data['train_y'][x]))\n",
    "plt.imshow(data['train_x'][x].reshape(DIM,DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  One hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 10), (10000, 10))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = 10\n",
    "id_mtx = np.identity(n_classes,dtype=np.float32)\n",
    "data['train_y'] = id_mtx[data['train_y']]\n",
    "data['test_y'] = id_mtx[data['test_y']]\n",
    "data['train_y'].shape, data['test_y'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['train_x'] = data['train_x'].astype(np.float32)/255\n",
    "data['test_x'] = data['test_x'].astype(np.float32)/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<BatchDataset shapes: ((?, 28, 28, 1), (?, 10)), types: (tf.float32, tf.float32)>,\n",
       " <BatchDataset shapes: ((?, 28, 28, 1), (?, 10)), types: (tf.float32, tf.float32)>,\n",
       " <BatchDataset shapes: ((?, 28, 28, 1), (?, 10)), types: (tf.float32, tf.float32)>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.variable_scope(\"dataset_gen\" ):    \n",
    "    dtrain_x = tf.data.Dataset.from_tensor_slices(data['train_x'])\n",
    "    dtrain_y = tf.data.Dataset.from_tensor_slices(data['train_y'])\n",
    "    dtrain = tf.data.Dataset.zip(( dtrain_x, dtrain_y )).batch(batch_size)\n",
    "    \n",
    "    dtest_x = tf.data.Dataset.from_tensor_slices(data['test_x'])\n",
    "    dtest_y = tf.data.Dataset.from_tensor_slices(data['test_y'])\n",
    "    dtest = tf.data.Dataset.zip(( dtest_x,dtest_y )).batch(batch_size)\n",
    "    \n",
    "    dvalid_x = tf.data.Dataset.from_tensor_slices(data['test_x'][:1000,:,:,:])\n",
    "    dvalid_y = tf.data.Dataset.from_tensor_slices(data['test_y'][:1000])\n",
    "    dvalid = tf.data.Dataset.zip(( dtest_x,dtest_y )).batch(batch_size)\n",
    "(dtrain,dtest,dvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"dataset_init\"):\n",
    "    iterator = tf.data.Iterator.from_structure(dtrain.output_types,dtrain.output_shapes)\n",
    "    get_batch = iterator.get_next()\n",
    "    #for train\n",
    "    dtrain_init = iterator.make_initializer(dtrain)\n",
    "    #for test\n",
    "    dtest_init = iterator.make_initializer(dtest)\n",
    "    #for validation\n",
    "    dvalid_init = iterator.make_initializer(dvalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10  \n",
    "probability_keep = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = tf.placeholder(shape=(),dtype=tf.bool,name='is_train')\n",
    "prob_keep = tf.placeholder(shape=(),dtype=tf.float32,name = 'probability_keep')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "current_model = model.CNN(get_batch[0], is_train, prob_keep)\n",
    "param_info = current_model.total_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits=current_model.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_lr = 1e-3\n",
    "end_lr = 5e-3\n",
    "decay_steps = 10000\n",
    "lr = tf.train.exponential_decay(start_lr,global_step, decay_steps, 0.96, staircase=True)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=get_batch[1],logits=logits))\n",
    "tf.summary.scalar(\"losses\",loss)\n",
    "tf.summary.scalar(\"learning_rate\",lr)\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    optimizer = tf.train.AdamOptimizer(lr).minimize(loss, global_step=global_step)\n",
    "predictions = tf.argmax(logits,axis=1)\n",
    "labels = tf.argmax(get_batch[1],axis=1)\n",
    "equality = tf.equal(predictions,labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(equality,tf.float32))\n",
    "conf_mtx=tf.confusion_matrix(labels=tf.argmax(get_batch[1],axis=1),predictions=tf.argmax(logits,axis=1),num_classes=n_classes)\n",
    "tf.summary.scalar(\"accuracy\",accuracy)\n",
    "#tf.summary.text(\"confusion_matrix\", tf.cast(conf_mtx,tf.string))\n",
    "ginit_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = 'floyd/home/logs'\n",
    "print(log_dir)\n",
    "t_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dataset(sess, init_op, variables, feed_dict, train=False):\n",
    "    sess.run(init_op)\n",
    "    logs = []\n",
    "    itr = 1\n",
    "    #stack = np.random.rand(1,n_classes)\n",
    "    #stack = []\n",
    "    stack = np.zeros((10,10),dtype=np.int32)\n",
    "    while True:\n",
    "        try:\n",
    "            res = sess.run(variables, feed_dict=feed_dict)\n",
    "            logs.append(res[0]) # first element of the result is either accuracy or loss\n",
    "            if train:\n",
    "                writer.add_summary(res[-1],i)\n",
    "                itr+=1\n",
    "                if itr%100 == 0:\n",
    "                    print(\"batch :{}, loss :{:.3f}, accuracy :{:.3f}\".format(itr,res[0],res[2]))\n",
    "            else:\n",
    "                #stack = np.vstack((stack,res[-1]))\n",
    "                #stack.append(res[-1])\n",
    "                stack = stack + res[-1]\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            return logs,stack\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_log_acc = []\n",
    "train_log_acc=[]\n",
    "test_acc=[]\n",
    "train_acc=[]\n",
    "with tf.Session() as sess:\n",
    "    writer.add_graph(sess.graph)\n",
    "    sess.run(ginit_op)\n",
    "    train_log_loss = []\n",
    "    valid_log_loss = []\n",
    "    for i in range(1,epochs+1):\n",
    "        print(\"training epoch : {}\".format(i))\n",
    "        sess.run(dtrain_init)\n",
    "        feed_dict={is_train:True,prob_keep:0.3}\n",
    "        variables = [loss,optimizer,accuracy,t_summary]\n",
    "        l,_  = run_dataset(sess, dtrain_init, variables, feed_dict, True)\n",
    "        train_log_loss.append(l)\n",
    "        \n",
    "        variables = [accuracy,conf_mtx]\n",
    "        feed_dict = {is_train:False,prob_keep:1.0}\n",
    "        acc,_ = run_dataset(sess, dvalid_init, variables, feed_dict)\n",
    "        acc = np.array(acc)\n",
    "        valid_log_acc.append(acc.mean())\n",
    "        print(\"average validation accuracy :{:.2f}\".format(acc.mean()))\n",
    "    \n",
    "    #for test dataset\n",
    "    variables = [accuracy,conf_mtx]\n",
    "    feed_dict = {is_train:False,prob_keep:1.0}\n",
    "    acc, test_stack = run_dataset(sess, dtest_init, variables, feed_dict)\n",
    "    test_acc = np.array(acc)\n",
    "    print(\"average test accuracy :{:.2f}\".format(test_acc.mean()))\n",
    "    \n",
    "    # for train dataset\n",
    "    variables = [accuracy,conf_mtx]\n",
    "    feed_dict = {is_train:False,prob_keep:1.0}\n",
    "    acc, train_stack = run_dataset(sess, dtrain_init, variables, feed_dict)\n",
    "    train_acc = np.array(acc)\n",
    "    print(\"average train accuracy :{:.2f}\".format(train_acc.mean()))\n",
    "    plt.plot(valid_log_acc,label='valid_acc',color='g')\n",
    "    train_log_loss = np.array(train_log_loss).reshape(-1)\n",
    "    x = np.arange(1,train_log_loss.shape[0]+1)/500\n",
    "    plt.plot(x,train_log_loss,label='train loss', color='r')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = dict()\n",
    "output_dict['batch_size'] = batch_size\n",
    "output_dict['epochs'] = epochs\n",
    "output_dict['loss'] = np.array(train_log_loss)\n",
    "output_dict['accuracy'] = np.array(valid_log_acc)\n",
    "output_dict['test_accuracy'] = np.array(test_acc)\n",
    "output_dict['train_accuracy'] = np.array(train_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.save('data_info_epochs_1.npy',output_dict)\n",
    "np.savetxt('train_stack.csv',train_stack[1:],delimiter=',')\n",
    "np.savetxt('test_stack.csv',test_stack[1:],delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save('/floyd/home/data_info_epoch_1.npy',output_dict)\n",
    "np.savetxt('/floyd/home/train_stack.csv',train_stack,delimiter=',')\n",
    "np.savetxt('/floyd/home/test_stack.csv',test_stack,delimiter=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
