{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "from cnn_model import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 10\n",
      "19 10\n",
      "19 10\n",
      "19 10\n"
     ]
    }
   ],
   "source": [
    "from prep_bs import mnist_funct, fashion_funct, merger\n",
    "mnist = mnist_funct()\n",
    "fashion = fashion_funct()\n",
    "merged = merger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['train_x', 'train_y', 'test_x', 'test_y']),\n",
       " dict_keys(['train_x', 'train_y', 'test_x', 'test_y']))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.keys(),fashion.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fashion \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "label  - description\n",
    "| --- | --- |\n",
    "| 10 | T-shirt/top |\n",
    "| 11 | Trouser |\n",
    "| 12 | Pullover |\n",
    "| 13 | Dress |\n",
    "| 14 | Coat |\n",
    "| 15 | Sandal |\n",
    "| 16 | Shirt |\n",
    "| 17 | Sneaker |\n",
    "| 18 | Bag |\n",
    "| 19 | Ankle boot |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {10:'T-shirt/top',11:'Trouser',12:'Pullover',13:'Dress',14:'Coat',15:'Sandal',16:'Shirt',17:'Sneaker',18:'Bag',19:'Ankle boot'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    labels[i] = str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: 'T-shirt/top',\n",
       " 11: 'Trouser',\n",
       " 12: 'Pullover',\n",
       " 13: 'Dress',\n",
       " 14: 'Coat',\n",
       " 15: 'Sandal',\n",
       " 16: 'Shirt',\n",
       " 17: 'Sneaker',\n",
       " 18: 'Bag',\n",
       " 19: 'Ankle boot',\n",
       " 0: '0',\n",
       " 1: '1',\n",
       " 2: '2',\n",
       " 3: '3',\n",
       " 4: '4',\n",
       " 5: '5',\n",
       " 6: '6',\n",
       " 7: '7',\n",
       " 8: '8',\n",
       " 9: '9'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train and test for cummulative dataset (shuffled) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<BatchDataset shapes: ((?, 28, 28, 1), (?, 20)), types: (tf.float32, tf.float32)>,\n",
       " <BatchDataset shapes: ((?, 28, 28, 1), (?, 20)), types: (tf.float32, tf.float32)>,\n",
       " <BatchDataset shapes: ((?, 28, 28, 1), (?, 20)), types: (tf.float32, tf.float32)>,\n",
       " <BatchDataset shapes: ((?, 28, 28, 1), (?, 20)), types: (tf.float32, tf.float32)>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "dtrain_x = tf.data.Dataset.from_tensor_slices(merged['train_x'])\n",
    "dtrain_y = tf.data.Dataset.from_tensor_slices(merged['train_y']).map(lambda x: tf.one_hot(x,20),num_parallel_calls=4)\n",
    "train_data = tf.data.Dataset.zip((dtrain_x,dtrain_y)).shuffle(1200).repeat().batch(batch_size)\n",
    "#one shot train\n",
    "train_one_shot = tf.data.Dataset.zip((dtrain_x,dtrain_y)).batch(batch_size)\n",
    "#test\n",
    "dtest_x = tf.data.Dataset.from_tensor_slices(merged['test_x'])\n",
    "dtest_y = tf.data.Dataset.from_tensor_slices(merged['test_y']).map(lambda x: tf.one_hot(x,20),num_parallel_calls=4)\n",
    "test_data = tf.data.Dataset.zip((dtest_x,dtest_y)).batch(batch_size)\n",
    "#valid mnist\n",
    "dvalid_mnist_x = tf.data.Dataset.from_tensor_slices(mnist['test_x'][:1001])\n",
    "dvalid_mnist_y = tf.data.Dataset.from_tensor_slices(mnist['test_y'][:1001]).map(lambda x: tf.one_hot(x,20),num_parallel_calls=4)\n",
    "valid_mnist_data =  tf.data.Dataset.zip((dvalid_mnist_x,dvalid_mnist_y)).repeat().batch(batch_size)\n",
    "#valid fashion\n",
    "dvalid_fashion_x = tf.data.Dataset.from_tensor_slices(fashion['test_x'][:1001])\n",
    "dvalid_fashion_y = tf.data.Dataset.from_tensor_slices(fashion['test_y'][:1001]).map(lambda x: tf.one_hot(x,20),num_parallel_calls=4)\n",
    "valid_fashion_data =  tf.data.Dataset.zip((dvalid_fashion_x,dvalid_fashion_y)).repeat().batch(batch_size)\n",
    "\n",
    "(train_data,test_data,valid_mnist_data,valid_fashion_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### individual test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mnist train\n",
    "mnist_train_x = tf.data.Dataset.from_tensor_slices(mnist['train_x'])\n",
    "mnist_train_y = tf.data.Dataset.from_tensor_slices(mnist['train_y']).map(lambda x: tf.one_hot(x,20),num_parallel_calls=4)\n",
    "mnist_train = tf.data.Dataset.zip((mnist_train_x,mnist_train_y)).shuffle(1000).batch(batch_size)\n",
    "#mnist test\n",
    "mnist_test_x = tf.data.Dataset.from_tensor_slices(mnist['test_x'])\n",
    "mnist_test_y = tf.data.Dataset.from_tensor_slices(mnist['test_y']).map(lambda x: tf.one_hot(x,20),num_parallel_calls=4)\n",
    "mnist_test = tf.data.Dataset.zip((mnist_test_x,mnist_test_y)).batch(batch_size)\n",
    "#fashion train\n",
    "fashion_train_x = tf.data.Dataset.from_tensor_slices(fashion['train_x'])\n",
    "fashion_train_y = tf.data.Dataset.from_tensor_slices(fashion['train_y']).map(lambda x: tf.one_hot(x,20),num_parallel_calls=4)\n",
    "fashion_train = tf.data.Dataset.zip((fashion_train_x,fashion_train_y)).batch(batch_size)\n",
    "#fashion test\n",
    "fashion_test_x = tf.data.Dataset.from_tensor_slices(fashion['test_x'])\n",
    "fashion_test_y = tf.data.Dataset.from_tensor_slices(fashion['test_y']).map(lambda x: tf.one_hot(x,20),num_parallel_calls=4)\n",
    "fashion_test =tf.data.Dataset.zip((fashion_test_x,fashion_test_y)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pipline iterator initializer\n",
    "iterator = tf.data.Iterator.from_structure(train_data.output_types,train_data.output_shapes)\n",
    "get_batch = iterator.get_next()\n",
    "\n",
    "#train data initiallizer shuffled\n",
    "train_init = iterator.make_initializer(train_data)\n",
    "#test data initializer shuffled\n",
    "test_init = iterator.make_initializer(test_data)\n",
    "#train one shot\n",
    "train_1s_init = iterator.make_initializer(train_one_shot)\n",
    "# valid data mnist\n",
    "valid_mnist_init = iterator.make_initializer(valid_mnist_data)\n",
    "# valid data fashion\n",
    "valid_fashion_init = iterator.make_initializer(valid_fashion_data)\n",
    "#fashoin train data initializer\n",
    "fashion_train_init = iterator.make_initializer(fashion_train)\n",
    "# fashion test data initializer \n",
    "fashion_test_init = iterator.make_initializer(fashion_test)\n",
    "#mnist train data initializer\n",
    "mnist_train_init = iterator.make_initializer(mnist_train)\n",
    "#mnist test data initializer\n",
    "mnist_test_init = iterator.make_initializer(mnist_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "param:\n",
    "param[0]  -  channel\n",
    "param[1]  -  filter1_size,# filters\n",
    "param[2]  -  filter2_size,# filters\n",
    "param[3]  -  dense size\n",
    "param[4]  -  output_size\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param=[]\n",
    "param.append(1)\n",
    "param.append([5,64])\n",
    "param.append([5,64])\n",
    "param.append(1000)\n",
    "param.append(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = tf.placeholder(shape=(),dtype=tf.bool)\n",
    "prob_keep = tf.placeholder(shape=(),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = model(get_batch[0],is_train,prob_keep,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "varname : conv_1/weight:0 [Dimension(5), Dimension(5), Dimension(1), Dimension(64)]\n",
      "varname : conv_1/bias:0 [Dimension(64)]\n",
      "varname : conv_2/weight:0 [Dimension(5), Dimension(5), Dimension(64), Dimension(64)]\n",
      "varname : conv_2/bias:0 [Dimension(64)]\n",
      "varname : dense/kernel:0 [Dimension(3136), Dimension(1000)]\n",
      "varname : dense/bias:0 [Dimension(1000)]\n",
      "varname : dense_1/kernel:0 [Dimension(1000), Dimension(20)]\n",
      "varname : dense_1/bias:0 [Dimension(20)]\n",
      "total number of trainable parameter : 3261148\n"
     ]
    }
   ],
   "source": [
    "logits = cnn.logits\n",
    "info = cnn.total_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_1/BiasAdd:0' shape=(?, 20) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=get_batch[1],logits=logits))\n",
    "\n",
    "tf.summary.scalar(\"losses\",loss)\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "#get accuracy\n",
    "predictions = tf.argmax(logits,axis=1)\n",
    "equality = tf.equal(predictions,tf.argmax(get_batch[1],axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(equality,tf.float32))\n",
    "tf.summary.scalar(\"accuracy\",accuracy)\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/output/logs\n"
     ]
    }
   ],
   "source": [
    "log_dir = '/output/logs'\n",
    "print(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(log_dir)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "epochs = int(epochs*120000/batch_size)\n",
    "probability_keep = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-9870fb07f38a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#saver.restore(sess,log_dir+'/curr_model.ckpt')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtrain_log_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py\u001b[0m in \u001b[0;36madd_graph\u001b[1;34m(self, graph, global_step, graph_def)\u001b[0m\n\u001b[0;32m    211\u001b[0m                       \"or the deprecated `GraphDef`\")\n\u001b[0;32m    212\u001b[0m     \u001b[1;31m# Finally, add the graph_def to the summary writer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_graph_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_graph_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_write_plugin_assets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py\u001b[0m in \u001b[0;36m_add_graph_def\u001b[1;34m(self, graph_def, global_step)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_add_graph_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m     \u001b[0mgraph_bytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m     \u001b[0mevent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevent_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgraph_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mSerializeToString\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m           'Message %s is missing required fields: %s' % (\n\u001b[0;32m   1041\u001b[0m           self.DESCRIPTOR.full_name, ','.join(self.FindInitializationErrors())))\n\u001b[1;32m-> 1042\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializePartialToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m   \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSerializeToString\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mSerializePartialToString\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mSerializePartialToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m   \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializePartialToString\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSerializePartialToString\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mInternalSerialize\u001b[1;34m(self, write_bytes, deterministic)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1071\u001b[1;33m         \u001b[0mfield_descriptor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m         \u001b[0mwrite_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\internal\\encoder.py\u001b[0m in \u001b[0;36mEncodeRepeatedField\u001b[1;34m(write, value, deterministic)\u001b[0m\n\u001b[0;32m    758\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m         \u001b[0mlocal_EncodeVarint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m         \u001b[0melement\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    761\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mEncodeRepeatedField\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\internal\\python_message.py\u001b[0m in \u001b[0;36mInternalSerialize\u001b[1;34m(self, write_bytes, deterministic)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mfield_descriptor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_value\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mListFields\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1071\u001b[1;33m         \u001b[0mfield_descriptor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mtag_bytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_bytes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unknown_fields\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m         \u001b[0mwrite_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\internal\\encoder.py\u001b[0m in \u001b[0;36mEncodeField\u001b[1;34m(write, value, deterministic)\u001b[0m\n\u001b[0;32m    823\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue_keys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m       \u001b[0mentry_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmessage_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concrete_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m       \u001b[0mencode_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mEncodeField\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\google\\protobuf\\internal\\encoder.py\u001b[0m in \u001b[0;36mEncodeField\u001b[1;34m(write, value, deterministic)\u001b[0m\n\u001b[0;32m    763\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mEncodeField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m       \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m       \u001b[0mlocal_EncodeVarint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_InternalSerialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mEncodeField\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "duration = 0\n",
    "log_mnist_test = 0\n",
    "log_fashion_test = 0\n",
    "with tf.Session() as sess:\n",
    "    #saver.restore(sess,log_dir+'/curr_model.ckpt')\n",
    "    writer.add_graph(sess.graph)\n",
    "    sess.run(init_op)\n",
    "    train_log_loss = []\n",
    "    train_log_mnist_acc = []\n",
    "    train_log_fashion_acc = []\n",
    "    \n",
    "    for i in range(1,epochs+1):\n",
    "        sess.run(train_init)\n",
    "        start_time = time.time()\n",
    "        l,_,acc,s = sess.run([loss,optimizer,accuracy,t_summary],feed_dict={is_train:True,prob_keep:probability_keep})\n",
    "        duration = duration + time.time() - start_time\n",
    "        writer.add_summary(s,i)\n",
    "        # history of loss and accuracy while training\n",
    "        train_log_loss.append(l)\n",
    "        #print(\"running epoch  : {}\".format(i))\n",
    "        if i%100==0:\n",
    "            print(\"Epoch :{}, loss :{:.3f}, accuracy :{:.3f}\".format(i,l,acc))\n",
    "            save_path = saver.save(sess,log_dir+'/curr_model.ckpt')\n",
    "            \n",
    "            test_iter = 10\n",
    "            mnist_avg_acc = 0\n",
    "            sess.run(valid_mnist_init)\n",
    "            for _ in range(test_iter):\n",
    "                acc = sess.run([accuracy],feed_dict = {is_train:False,prob_keep:1.0})\n",
    "                mnist_avg_acc+=acc[0]\n",
    "            mnist_avg_acc=mnist_avg_acc*100.0/test_iter\n",
    "            print(\"validation mnist accuracy :{:.3f}\".format(mnist_avg_acc))\n",
    "            train_log_mnist_acc.append(mnist_avg_acc)\n",
    "        \n",
    "            fashion_avg_acc = 0\n",
    "            sess.run(valid_fashion_init)\n",
    "            for _ in range(test_iter):\n",
    "                acc = sess.run([accuracy],feed_dict = {is_train:False,prob_keep:1.0})\n",
    "                fashion_avg_acc+=acc[0]\n",
    "            fashion_avg_acc=fashion_avg_acc*100.0/test_iter\n",
    "            print(\"validation fashion accuracy :{:.3f}\".format(fashion_avg_acc))\n",
    "            train_log_fashion_acc.append(fashion_avg_acc)\n",
    "            if mnist_avg_acc>=97 and i>=500:\n",
    "                break\n",
    "    \n",
    "    \n",
    "    #final accuracy on test data\n",
    "    i=0\n",
    "    t_acc = 0\n",
    "    sess.run(test_init)\n",
    "    while True:\n",
    "        try:\n",
    "            i+=1\n",
    "            #100 --> batch size for one_shot_iter is 100\n",
    "            acc = sess.run([accuracy],feed_dict = {is_train:False,prob_keep:1.0})\n",
    "            t_acc+=acc[0]\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            i = i-1\n",
    "            print(\"Test Accuracy: {}\".format(t_acc*100.0/(i)))\n",
    "            total_test_acc = t_acc*100.0/i\n",
    "            plt.plot(train_log_loss,label='loss_train',color='r')\n",
    "            plt.plot(train_log_mnist_acc,label='acc_mnist',color='b')\n",
    "            plt.plot(train_log_fashion_acc,label='acc_fashion',color='g')\n",
    "            plt.xlabel(\"epochs\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            break\n",
    "    \n",
    "    #total train accuracy\n",
    "    i=0\n",
    "    t_acc = 0\n",
    "    sess.run(train_1s_init)\n",
    "    while True:\n",
    "        try:\n",
    "            i+=1\n",
    "            #100 --> batch size for one_shot_iter is 100\n",
    "            acc = sess.run([accuracy],feed_dict = {is_train:False,prob_keep:1.0})\n",
    "            t_acc+=acc[0]\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            i = i-1\n",
    "            print(\"total train Accuracy: {}\".format(t_acc*100.0/(i)))\n",
    "            total_train_acc = t_acc*100.0/i\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "    #accuracy for mnist data\n",
    "    i=0\n",
    "    t_acc = 0\n",
    "    sess.run(mnist_test_init)\n",
    "    while True:\n",
    "        try:\n",
    "            i+=1\n",
    "            #100 --> batch size for one_shot_iter is 100\n",
    "            acc = sess.run([accuracy],feed_dict = {is_train:False,prob_keep:1.0})\n",
    "            t_acc+=acc[0]\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            i = i-1\n",
    "            log_mnist_test = t_acc*100/i\n",
    "            print(\"MNIST Test Accuracy: {}\".format(t_acc*100/(i)))\n",
    "            break\n",
    "    #train mnist\n",
    "    i=0\n",
    "    t_acc = 0\n",
    "    sess.run(mnist_train_init)\n",
    "    while True:\n",
    "        try:\n",
    "            i+=1\n",
    "            #100 --> batch size for one_shot_iter is 100\n",
    "            acc = sess.run([accuracy],feed_dict = {is_train:False,prob_keep:1.0})\n",
    "            t_acc+=acc[0]\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            i = i-1\n",
    "            log_mnist_train = t_acc*100/i\n",
    "            print(\"MNIST train Accuracy: {}\".format(t_acc*100/(i)))\n",
    "            break\n",
    "    \n",
    "    #accuracy for fashion data\n",
    "    i=0\n",
    "    t_acc = 0\n",
    "    sess.run(fashion_test_init)\n",
    "    while True:\n",
    "        try:\n",
    "            i+=1\n",
    "            #100 --> batch size for one_shot_iter is 100\n",
    "            acc = sess.run([accuracy],feed_dict = {is_train:False,prob_keep:1.0})\n",
    "            t_acc+=acc[0]\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            i = i-1\n",
    "            log_fashion_test = t_acc*100/i\n",
    "            print(\"fashion test Accuracy: {}\".format(t_acc*100.0/(i)))\n",
    "            break\n",
    "    # train\n",
    "    i=0\n",
    "    t_acc = 0\n",
    "    sess.run(fashion_train_init)\n",
    "    while True:\n",
    "        try:\n",
    "            i+=1\n",
    "            #100 --> batch size for one_shot_iter is 100\n",
    "            acc = sess.run([accuracy],feed_dict = {is_train:False,prob_keep:1.0})\n",
    "            t_acc+=acc[0]\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"i : \"+str(i))\n",
    "            i = i-1\n",
    "            log_fashion_train = t_acc*100/i\n",
    "            print(\"fashion Train Accuracy: {}\".format(t_acc*100.0/(i)))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save log files\n",
    "log=dict()\n",
    "log['batch_size'] = batch_size\n",
    "log['epochs'] = epochs\n",
    "log['param_info'] = info\n",
    "log['duration'] = np.array(duration)\n",
    "log['loss'] = np.array(train_log_loss)\n",
    "log['total_train_acc'] = total_train_acc\n",
    "log['total_test_acc'] = total_test_acc\n",
    "log['mnist_valid'] = np.array(train_log_mnist_acc)\n",
    "log['fashion_valid'] = np.array(train_log_fashion_acc)\n",
    "log['mnist_train_test'] = np.array([log_mnist_train,log_mnist_test])\n",
    "log['fashion_train_test'] = np.array([log_fashion_train,log_fashion_test])\n",
    "np.save('/output/proj_shuffle_test1e.npy',log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
